{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'> *Uczenie maszynowe* </font>\n",
    "\n",
    "## TEMAT 4: Klasyfikatory (NM, NN, kNN, SVM)\n",
    "\n",
    "<font color='orange'> Prowadząca: dr inż. Urszula Libal </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Nearest Mean (NM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Pierwsza próba klasyfikacji pojedynczego obiektu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZAD.1 Nearest Mean\n",
    "\n",
    "# Uwaga: w dokumentacji scikit-learn nazywany klasyfikatorem Nearest Centroid\n",
    "\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) # cechy\n",
    "y = np.array([1, 1, 1, 2, 2, 2]) # klasy\n",
    "\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "nm = NearestCentroid()\n",
    "nm.fit(X, y) # uczenie\n",
    "\n",
    "print(\"Obiekt [-0.8, -1] klasyfikujemy do\", nm.predict([[-0.8, -1]])) # klasyfikujemy obiekt o cechach [-0.8, -1]\n",
    "print(\"Obiekt [1.2, 0] klasyfikujemy do\", nm.predict([[1.2, 0]])) # klasyfikujemy obiekt o cechach [1.2, 0]\n",
    "print(\"Obiekt [3.2, 1.4] klasyfikujemy do\", nm.predict([[3.2, 1.4]])) # klasyfikujemy obiekt o cechach [3.2, 1.4]\n",
    "\n",
    "\n",
    "\n",
    "# Ile wynoszą średnie w klasach?\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Obszary decyzyjne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZAD.2 Obszary decyzyjne (metryka euklidesowa)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "feature_1, feature_2 = np.meshgrid(\n",
    "    np.linspace(-4, 4),\n",
    "    np.linspace(-3, 3)\n",
    ")\n",
    "grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n",
    "\n",
    "\n",
    "nm = NearestCentroid().fit(X, y) # domyślna metryka euklidesowa\n",
    "y_pred = np.reshape(nm.predict(grid), feature_1.shape)\n",
    "display = DecisionBoundaryDisplay(\n",
    "    xx0=feature_1, xx1=feature_2, response=y_pred\n",
    ")\n",
    "display.plot()\n",
    "\n",
    "display.ax_.scatter(\n",
    "    X[:, 0], X[:, 1], c=y, edgecolor=\"black\"\n",
    ")\n",
    "\n",
    "plt.plot(nm.centroids_[0][0], nm.centroids_[0][1], 'xr') # średnia w klasie 1\n",
    "plt.plot(nm.centroids_[1][0], nm.centroids_[1][1], 'or') # średnia w klasie 2\n",
    "\n",
    "plt.title(\"Obszary decyzyjne dla NearestMean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZAD.3 Obszary decyzyjne (metryka Manhattan)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "feature_1, feature_2 = np.meshgrid(\n",
    "    np.linspace(-4, 4),\n",
    "    np.linspace(-3, 3)\n",
    ")\n",
    "grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n",
    "\n",
    "\n",
    "nm2 = NearestCentroid(metric='manhattan').fit(X, y)\n",
    "y_pred = np.reshape(nm2.predict(grid), feature_1.shape)\n",
    "display = DecisionBoundaryDisplay(\n",
    "    xx0=feature_1, xx1=feature_2, response=y_pred\n",
    ")\n",
    "display.plot()\n",
    "\n",
    "display.ax_.scatter(\n",
    "    X[:, 0], X[:, 1], c=y, edgecolor=\"black\"\n",
    ")\n",
    "\n",
    "plt.plot(nm2.centroids_[0][0], nm2.centroids_[0][1], 'xr') # średnia w klasie 1\n",
    "plt.plot(nm2.centroids_[1][0], nm2.centroids_[1][1], 'or') # średnia w klasie 2\n",
    "\n",
    "plt.title(\"Obszary decyzyjne dla NearestMean (metryka manhattan)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównaj położenie centrów klas dla metryki euklidesowej (zad.2) i metryki manhattan (zad.3).\n",
    "\n",
    "\n",
    "# centra dla metryki euklidesowej to\n",
    "\n",
    "# ...\n",
    "\n",
    "# centra dla metryki manhattan\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZAD.4 Centra (dane Digits)\n",
    "\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "model = NearestCentroid().fit(digits.data, digits.target)\n",
    "model.centroids_.shape\n",
    "\n",
    "for nr_klasy in range(10):\n",
    "    print(\"\\nCentrum dla klasy\",nr_klasy)\n",
    "    plt.matshow(model.centroids_[nr_klasy].reshape(8,8), cmap='Greys') # 8x8 pixeli\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZAD.5 Macierz pomyłek (dane Digits)\n",
    "\n",
    "\n",
    "# Oblicz macierz pomyłek dla klasyfikatora NearestCentroid oraz danych digits.\n",
    "\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Nearest Neighbor (NN), k-Nearest Neighbors (kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ZAD.6 Obszary decyzyjne (dane Iris)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "feature_1, feature_2 = np.meshgrid(\n",
    "    np.linspace(iris.data[:, 0].min(), iris.data[:, 0].max()),\n",
    "    np.linspace(iris.data[:, 1].min(), iris.data[:, 1].max())\n",
    ")\n",
    "grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n",
    "\n",
    "for k in [1,3,5,7,21]:\n",
    "    model = KNeighborsClassifier(n_neighbors=k).fit(iris.data[:, :2], iris.target) # k to liczba najbliższych sąsiadów\n",
    "    y_pred = np.reshape(model.predict(grid), feature_1.shape)\n",
    "    display = DecisionBoundaryDisplay(xx0=feature_1, xx1=feature_2, response=y_pred)\n",
    "    display.plot(cmap=\"cool\")\n",
    "    display.ax_.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target, edgecolor=\"black\")\n",
    "    plt.title('Obszary decyzyjne dla kNN (k = {})'.format(k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZAD.7 Obszary decyzyjne (dane Iris)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "iris = load_iris()\n",
    "feature_1, feature_2 = np.meshgrid(\n",
    "    np.linspace(iris.data[:, 0].min(), iris.data[:, 0].max()),\n",
    "    np.linspace(iris.data[:, 1].min(), iris.data[:, 1].max())\n",
    ")\n",
    "grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n",
    "\n",
    "model = SVC().fit(iris.data[:, :2], iris.target)\n",
    "y_pred = np.reshape(model.predict(grid), feature_1.shape)\n",
    "display = DecisionBoundaryDisplay(xx0=feature_1, xx1=feature_2, response=y_pred)\n",
    "display.plot(cmap=\"cool\")\n",
    "display.ax_.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target, edgecolor=\"black\")\n",
    "plt.title('Obszary decyzyjne dla SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porównanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZAD.8  Problematyczne rozłożenie danych w klasach\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# Modified for classes by Urszula Libal\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "names = [\n",
    "    \"Naive Bayes\",\n",
    "    \"LDA\",\n",
    "    \"QDA\",\n",
    "    \"NM\",\n",
    "    \"NN\",\n",
    "    \"3NN\",\n",
    "    \"5NN\",\n",
    "    \"SVM\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    \n",
    "    # Zdefiniuj wszytkie klasyfikatory podane na liście \"names\"\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    ")\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [\n",
    "    make_moons(noise=0.3, random_state=0),\n",
    "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "    linearly_separable,\n",
    "]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42\n",
    "    )\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "    # Plot the testing points\n",
    "    ax.scatter(\n",
    "        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n",
    "    )\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        DecisionBoundaryDisplay.from_estimator(\n",
    "            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "        )\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(\n",
    "            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "        )\n",
    "        # Plot the testing points\n",
    "        ax.scatter(\n",
    "            X_test[:, 0],\n",
    "            X_test[:, 1],\n",
    "            c=y_test,\n",
    "            cmap=cm_bright,\n",
    "            edgecolors=\"k\",\n",
    "            alpha=0.6,\n",
    "        )\n",
    "\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(\n",
    "            x_max - 0.3,\n",
    "            y_min + 0.3,\n",
    "            (\"%.2f\" % score).lstrip(\"0\"),\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
